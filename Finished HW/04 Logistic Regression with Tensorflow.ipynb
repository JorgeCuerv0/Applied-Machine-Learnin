{"cells":[{"cell_type":"markdown","metadata":{"id":"MKsRDH5ZUdfasdv"},"source":["# Assignment 4"]},{"cell_type":"markdown","metadata":{"id":"tq4SMH_3wl-O"},"source":["### <span style=\"color:chocolate\"> Submission requirements </span>\n","\n","Your work will not be graded if your notebook doesn't include output. In other words, <span style=\"color:red\"> make sure to rerun your notebook before submitting to Gradescope </span> (Note: if you are using Google Colab: go to Edit > Notebook Settings  and uncheck Omit code cell output when saving this notebook, otherwise the output is not printed).\n","\n","Additional points may be deducted if these requirements are not met:\n","    \n","* Comment your code;\n","* Each graph should have a title, labels for each axis, and (if needed) a legend. Each graph should be understandable on its own;\n","* Try and minimize the use of the global namespace (meaning, keep things inside functions).\n","---"]},{"cell_type":"markdown","metadata":{"id":"6DnZbCvnwl-P"},"source":["### Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7X58hOMTUH-w"},"outputs":[],"source":["import numpy as np\n","from matplotlib import pyplot as plt\n","import pandas as pd\n","import seaborn as sns  # for nicer plots\n","sns.set(style=\"darkgrid\")  # default style\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import metrics\n","from keras.datasets import fashion_mnist\n","\n","tf.get_logger().setLevel('INFO')"]},{"cell_type":"markdown","metadata":{"id":"YP61jGeawl-Q"},"source":["---\n","### Step 1: Data ingestion"]},{"cell_type":"markdown","metadata":{"id":"rHLcriKWLRe4"},"source":["You'll train a binary classifier using the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. This consists of 70,000 grayscale images (28x28). Each image is associated with 1 of 10 classes. The dataset was split by the creators; there are 60,000 training images and 10,000 test images. Note also that Tensorflow includes a growing [library of datasets](https://www.tensorflow.org/datasets/catalog/overview) and makes it easy to load them in numpy arrays."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Evj4cIjowl-Q"},"outputs":[],"source":["# Load the Fashion MNIST dataset.\n","(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()"]},{"cell_type":"markdown","metadata":{"id":"FNETDC0swl-Q"},"source":["---\n","### Step 2: Exploratory Data Analysis (EDA)"]},{"cell_type":"markdown","metadata":{"id":"zrGQhA-gwl-Q"},"source":["Exploratory Data Analysis (EDA) and Data Preprocessing are often iterative processes that involve going back and forth to refine and improve the quality of data analysis and preparation. However, the specific order can vary depending on the project's requirements. In some cases, starting with EDA, as you see in this assignment, could be more useful, but there is no rigid rule dictating the sequence in all situations."]},{"cell_type":"markdown","metadata":{"id":"lqom5p1Kwl-Q"},"source":["### <span style=\"color:chocolate\">Exercise 1:</span> Getting to know your data (5 points)"]},{"cell_type":"markdown","metadata":{"id":"load_auto_data_set_code"},"source":["Complete the following tasks:\n","\n","1. Print the shapes and types of (X_train, Y_train) and (X_test, Y_test). Interpret the shapes (i.e., what do the numbers represent?). Hint: For types use the <span style=\"color:chocolate\">type()</span> function.\n","2. Define a list of strings of class names corresponding to each class in (Y_train, Y_test). Call this list label_names. Hint: Refer to the Fashion MNIST documentation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qPpAAmChwl-R"},"outputs":[],"source":["# YOUR CODE HERE\n","print(f\"X_train shape: {X_train.shape}\")\n","print(f\"X_train type: {type(X_train)}\\n\")\n","\n","print(f\"Y_train shape: {Y_train.shape}\")\n","print(f\"Y_train type: {type(Y_train)}\\n\")\n","\n","print(f\"X_test shape: {X_test.shape}\")\n","print(f\"X_test type: {type(X_test)}\\n\")\n","\n","print(f\"Y_test shape: {Y_test.shape}\")\n","print(f\"Y_test type: {type(Y_test)}\\n\")\n","\n","#define list of strings\n","label_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"]},{"cell_type":"markdown","source":["X_train is a NumPy array with 60,000 samples.Each sample is represented as a 28x28 matrix corresponding to each image pixel's\n","\n","Y_train is a NumPy array with 60,000 target values.\n","\n","X_test is a NumPy array with 10,000 samples.Each sample is represented as a 28x28 matrix corresponding to each image pixel's\n","\n","Y_test is a NumPy array with 10,000 target values."],"metadata":{"id":"gs-0Sf2J0q_G"}},{"cell_type":"markdown","metadata":{"id":"w9bTwJ62wl-R"},"source":["### <span style=\"color:chocolate\">Exercise 2:</span> Getting to know your data - cont'd (5 points)"]},{"cell_type":"markdown","metadata":{"id":"Og-OOJdUwl-R"},"source":["Fashion MNIST images have one of 10 possible labels (shown above)."]},{"cell_type":"markdown","metadata":{"id":"iWzMG3pzwl-R"},"source":["Complete the following tasks:\n","\n","1. Display the first 5 images in X_train for each class in Y_train, arranged in a 10x5 grid. Use the label_names list defined above;\n","2. Determine the minimum and maximum pixel values for images in the X_train dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H0jWDWVAwl-R"},"outputs":[],"source":["# Using list comprehension to make a dictionary of empty lists corresponding to each clothing label\n","indice_dict = {label: [] for label in label_names}\n","\n","for i, label in enumerate(Y_train):\n","  # Get number that corresponds to label, then pass it into our label names list to \"translate\" it into words then set it equal to our variable\n","  article_of_clothing = label_names[label]\n","\n","  # If the list of the article of clothing is bigger than 5 skip\n","  if len(indice_dict[article_of_clothing]) < 5:\n","\n","    #add the index to the article of clothing's list\n","    indice_dict[article_of_clothing].append(i)\n","\n","#making a plot that has 10 rows (1 for each label) 5 columns (1 for each index)\n","fig, axes = plt.subplots(nrows=10, ncols=5, figsize=(15, 30))\n","\n","#plotting\n","#loop through label_names list and store the name of the clothing in class_name\n","for i, class_name in enumerate(label_names):\n","  #loop through indice_dict[class_name] list and store the index of the type of clothing in index\n","  for j, index in enumerate(indice_dict[class_name]):\n","    #set ax to the current grid we are on\n","    ax = axes[i, j]\n","    #in the ax display the image that is stored in X_train by using the index we stored\n","    ax.imshow(X_train[index])\n","    #turn off the gridlines and numbers to make it look neater\n","    ax.axis('off')\n","    #put a tiltle for each plot\n","    ax.set_title(f\"{class_name} {j + 1}\")\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"cZpFxxStzSrP"},"source":["---\n","### Step 3: Data preprocessing"]},{"cell_type":"markdown","metadata":{"id":"dVJK18pdwl-R"},"source":["This step is essential for preparing this image data in a format that is suitable for ML algorithms."]},{"cell_type":"markdown","metadata":{"id":"yXi0arYLwl-R"},"source":["### <span style=\"color:chocolate\">Exercise 3:</span> Feature preprocessing (5 points)"]},{"cell_type":"markdown","metadata":{"id":"wqCNaQQLwl-R"},"source":["In the previous lab, the input data had just a few features. Here, we treat **every pixel value as a separate feature**, so each input example has 28x28 (784) features!"]},{"cell_type":"markdown","metadata":{"id":"499pvnvgwl-R"},"source":["In this exercise, you'll perform the following tasks:\n","\n","1. Normalize the pixel values in both X_train and X_test data so they range between 0 and 1;\n","2. For each image in X_train and X_test, flatten the 2-D 28x28 pixel array to a 1-D array of size 784. Hint: use the <span style=\"color:chocolate\">reshape()</span> method available in NumPy. Note that by doing so you will overwrite the original arrays;\n","3. Pint the shape of X_train and X_test arrays."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IPVBso9awl-R"},"outputs":[],"source":["# To normalize both X_train and X_test so that they range between 0 and 1\n","X_train_norm = X_train / 255.0\n","X_test_norm = X_test / 255.0\n","\n","# We use the reshape function in numpy to flatten the our df, we pass in our normalized df, and we specify that we want to keep the 6000 rows but we multiply 28*28 because those are the dementions of our images\n","X_train_norm_flat = X_train_norm.reshape(X_train.shape[0], -1)\n","X_test_norm_flat = X_test_norm.reshape(X_test.shape[0], -1)\n","\n","# Print the shapes of the flattened arrays\n","print(f\"Shape of X_train_norm_flat: {X_train_norm_flat.shape}\")\n","print(f\"Shape of X_test_norm_flat: {X_test_norm_flat.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"qpcZaYirwl-S"},"source":["### <span style=\"color:chocolate\">Exercise 4:</span> Label preprocessing (5 points)"]},{"cell_type":"markdown","metadata":{"id":"sTDjb8N7wl-S"},"source":["This assignment involves binary classification. Specifically, the objective is to predict whether an image belongs to the sneaker class (class 7) or not.\n","\n","Therefore, write code so that for each example in (Y_train, Y_test), the outcome variable is represented as follows:\n","* $y=1$, for sneaker class (positive examples), and\n","* $y=0$, for non-sneaker class (negative examples).\n","\n","Note: To avoid \"ValueError: assignment destination is read-only\", first create a copy of the (Y_train, Y_test) data and call the resulting arrays (Y_train, Y_test). Then overwrite the (Y_train, Y_test) arrays to create binary outcomes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FqFxbwj4wl-S"},"outputs":[],"source":["# Make copies of the original dataset for binary classification task.\n","Y_train_copy = np.copy(Y_train)\n","Y_test_copy = np.copy(Y_test)\n","\n","# create a new array of the same shape as Y_train_copy filled with zeros\n","Y_train_binary = np.zeros_like(Y_train_copy)\n","Y_test_binary = np.zeros_like(Y_test_copy)\n","\n","Y_train_binary = (Y_train == 7).astype(int)\n","Y_test_binary = (Y_test == 7).astype(int)\n","\n","# Printing first 10 values to check work\n","print(f\"Y_train_binary: {Y_train_binary[:10]}\")\n","print(f\"Y_test_binary: {Y_test_binary[:10]}\")"]},{"cell_type":"markdown","metadata":{"id":"5mwRf2x2wl-S"},"source":["### <span style=\"color:chocolate\">Exercise 5:</span> Data splits (10 points)"]},{"cell_type":"markdown","metadata":{"id":"jRn9koJ0wl-S"},"source":["Using the <span style=\"color:chocolate\">train_test_split()</span> method available in scikit-learn:\n","1. Retain 20% from the training data for validation purposes. Set random state to 1234. Name the resulting dataframes as follows: X_train_mini, X_val, Y_train_mini, Y_val.\n","2. Print the shape of each array."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W2ecyuX_wl-S"},"outputs":[],"source":["# Using train_test_split we passed in X_train, Y_train df's and set the test size to .20\n","X_train_mini, X_val, Y_train_mini, Y_val = train_test_split(X_train_norm_flat, Y_train_binary, test_size = 0.2, random_state = 1234)\n","\n","# Print the shapes of the flattened arrays\n","print(f\"Shape of X_train_norm_flat: {X_train_mini.shape}\")\n","print(f\"Shape of Y_train_mini: {Y_train_mini.shape}\")\n","print(f\"Shape of X_val: {X_val.shape}\")\n","print(f\"Shape of Y_val: {Y_val.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"Bcqc4fuuwl-S"},"source":["### <span style=\"color:chocolate\">Exercise 6:</span> Data shuffling (10 points)"]},{"cell_type":"markdown","metadata":{"id":"2axHLh24wl-S"},"source":["Since you'll be using Batch Gradient Descent (BGD) for training, it is important that **each batch is a random sample of the data** so that the gradient computed is representative.\n","\n","1. Use [integer array indexing](https://numpy.org/doc/stable/reference/arrays.indexing.html#integer-array-indexing) to re-order (X_train_mini, Y_train_mini) using a list of shuffled indices. In doing so, you will overwrite the arrays."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_wTsJRSCwl-S"},"outputs":[],"source":["np.random.seed(0)\n","\n","# generate a list of indices from 0 to the length of X_train_mini - 1.\n","index_list = np.arange(len(X_train_mini))\n","\n","#shuffle this list of indices\n","np.random.shuffle(index_list)\n","X_train_mini = X_train_mini[index_list]\n","Y_train_mini = Y_train_mini[index_list]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JpqHZD0lwl-S"},"source":["---\n","### Step 4: Exploratory Data Analysis (EDA) - cont'd"]},{"cell_type":"markdown","metadata":{"id":"XPvYuhuxwl-T"},"source":["Before delving into model training, let's further explore the raw feature values by comparing sneaker and non-sneaker training images."]},{"cell_type":"markdown","metadata":{"id":"LrRntvHiwl-T"},"source":["### <span style=\"color:chocolate\">Exercise 7:</span> Pixel distributions (10 points)"]},{"cell_type":"markdown","metadata":{"id":"uGKC64yGwl-T"},"source":["1. Identify all sneaker images in X_train_mini and calculate the mean pixel value for each sneaker image. Visualize these pixel values using a histogram. Print the mean pixel value across all sneaker images.\n","2. Identify all non-sneaker images in X_train_mini and calculate the mean pixel value for each non-sneaker image. Visualize these pixel values using a histogram. Print the mean pixel value across all non-sneaker images.\n","3. Based on the histogram results, assess whether there is any evidence suggesting that pixel values can be utilized to distinguish between sneaker and non-sneaker images. Justify your response.\n","\n","Notes: Make sure to provide a descriptive title and axis labels for each histogran. Make sure you utilize Y_train_mini to locate the sneaker and non-sneaker class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fgBjmFYqwl-T"},"outputs":[],"source":["# Use boolean indexing to create separate arrays for sneaker and non-sneaker images\n","sneaker_indices = Y_train_mini == 1\n","non_sneaker_indices = Y_train_mini == 0\n","\n","# Flatten the images to ensure each image is a 1D array\n","X_train_mini_flat = X_train_mini.reshape(X_train_mini.shape[0], -1)\n","\n","# Extract images\n","sneaker_images = X_train_mini_flat[sneaker_indices]\n","non_sneaker_images = X_train_mini_flat[non_sneaker_indices]\n","\n","# Calculate mean pixel values for each image (ensure the arrays are 2D)\n","sneaker_images_mean = sneaker_images.mean(axis=1)\n","non_sneaker_images_mean = non_sneaker_images.mean(axis=1)\n","\n","#visualize\n","plt.figure(figsize=(12,6))\n","\n","plt.subplot(1,2,1)\n","\n","plt.hist(sneaker_images_mean, bins = 30, alpha = 0.70, label = 'Sneaker')\n","plt.title(\"Histogram of Mean Pixel Values - Sneaker\")\n","plt.xlabel('Mean Pixel Value')\n","plt.ylabel('Frequency')\n","plt.legend()\n","\n","plt.subplot(1,2,2)\n","\n","plt.hist(sneaker_images_mean, bins = 30, alpha = 0.70, label = 'Non-Sneaker')\n","plt.title(\"Histogram of Mean Pixel Values - Non-Sneaker\")\n","plt.xlabel('Mean Pixel Value')\n","plt.ylabel('Frequency')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()\n","\n","mean_sneaker_pixel_value = sneaker_images_mean.mean()\n","mean_non_sneaker_pixel_value = non_sneaker_images_mean.mean()\n","\n","print(f'Mean pixel value for sneaker images: {mean_sneaker_pixel_value}')\n","print(f'Mean pixel value for non-sneaker images: {mean_non_sneaker_pixel_value}')\n","\n","#Based on the histogram results, assess whether there is any evidence suggesting that pixel values can be utilized to distinguish between sneaker and non-sneaker images. Justify your response.\n","# No, based on the results the pixel (approximately 72.93 for sneakers and 73.47 for non-sneakers) values between sneaker and non-sneaker are too close to call with certanty. The overlaping of histograms suggests that there isn't a clear distinction\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pT8JmFcIwl-T"},"source":["---\n","### Step 4: Modeling"]},{"cell_type":"markdown","metadata":{"id":"h_bwdD00wl-T"},"source":["### <span style=\"color:chocolate\">Exercise 8:</span> Baseline model (10 points)"]},{"cell_type":"markdown","metadata":{"id":"TGgQ-ASYiHbK"},"source":["When dealing with classification problems, a simple baseline is to select the *majority* class (the most common label in the training set) and use it as the prediction for all inputs.\n","\n","With this information in mind:\n","\n","1. What is the number of sneaker images in Y_train_mini?\n","2. What is the number of non-sneaker images in Y_train_mini?\n","3. What is the majority class in Y_train_mini?\n","4. What is the accuracy of a majority class classifier for Y_train_mini?\n","5. Implement a function that computes the Log Loss (binary cross-entropy) metric and use it to evaluate this baseline on both the mini train (Y_train_mini) and validation (Y_val) data. Use 0.1 as the predicted probability for your baseline (reflecting what we know about the original distribution of classes in the mini training data). Hint: for additional help, see the file ``04 Logistic Regression with Tensorflow_helpers.ipynb``."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G4aQxlo-wl-U"},"outputs":[],"source":["# Assuming sneaker_images and non_sneaker_images are correctly defined\n","sneaker_num = len(sneaker_images)\n","non_sneaker_num = len(non_sneaker_images)\n","\n","print(f\"Number of sneaker images in Y_train_mini: {sneaker_num}\")\n","print(f\"Number of non-sneaker images in Y_train_mini: {non_sneaker_num}\")\n","\n","# Determine the majority class\n","if sneaker_num > non_sneaker_num:\n","    majority_class = 1\n","    print(f\"Majority class is sneakers\")\n","else:\n","    majority_class = 0\n","    print(f\"Majority class is non-sneakers\")\n","\n","# Calculate the accuracy of a majority class classifier\n","accuracy = max(sneaker_num, non_sneaker_num) / (sneaker_num + non_sneaker_num)\n","print(f\"The accuracy is: {accuracy}\")\n","\n","# Log loss function: measures the performance of a classification model whose output is a probability value between 0 and 1\n","# It provides a measure of how well the predicted probabilities align with the actual outcomes.\n","# Penalization: It heavily penalizes wrong predictions that are confident (i.e., predicting a high probability for the wrong class).\n","def log_loss(labels, predicted_probabilities):\n","  \"\"\"Build a Log Loss function model.\n","\n","  Args:\n","    labels: Actual labels (binary).\n","    predicted_probabilities: Predicted probabilities.\n","\n","  Returns:\n","    model: A tf.keras model (graph).\n","  \"\"\"\n","  total_log_loss = 0\n","\n","  for i in labels:\n","    #Check if the label is a sneaker (represented by 1).\n","    if i == 1:\n","      total_log_loss += -np.log(predicted_probabilities)\n","    else:\n","      total_log_loss += -np.log(1 - predicted_probabilities)\n","\n","  average_log_loss = total_log_loss / len(labels)\n","\n","  return average_log_loss\n","\n","# Calculate log loss for Y_train_mini and Y_val\n","train_log_loss = log_loss(Y_train_mini, 0.1)\n","val_log_loss = log_loss(Y_val, 0.1)\n","\n","print(f\"Log Loss on training data: {train_log_loss}\")\n","print(f\"Log Loss on validation data: {val_log_loss}\")"]},{"cell_type":"markdown","metadata":{"id":"IHBLvS5Gwl-U"},"source":["### <span style=\"color:chocolate\">Exercise 9:</span> Improvement over Baseline with TensorFlow (10 points)"]},{"cell_type":"markdown","metadata":{"id":"CVJ_lSsMwl-U"},"source":["Let's use TensorFlow to train a binary logistic regression model much like you did in the previous assignment. The goal here is to build a ML model to improve over the baseline classifier.\n","\n","1. Fill in the <span style=\"color:green\">NotImplemented</span> parts of the build_model() function below by following the instructions provided as comments. Hint: the activation function, the loss, and the evaluation metric are different compared to the linear regression model;\n","2. Build and compile a model using the build_model() function and the (X_train_mini, Y_train_mini) data. Set learning_rate = 0.0001. Call the resulting object *model_tf*.\n","3. Train *model_tf* using the (X_train_mini, Y_train_mini) data. Set num_epochs = 5 and batch_size=32. Pass the (X_val, Y_val) data for validation. Hint: see the documentation behind the [tf.keras.Model.fit()](https://bcourses.berkeley.edu/courses/1534588/files/88733489?module_item_id=17073646) method.\n","3. Generate a plot (for the mini training and validation data) with the loss values on the y-axis and the epoch number on the x-axis for visualization. Make sure to include axes name and title. Hint: check what the [tf.keras.Model.fit()](https://bcourses.berkeley.edu/courses/1534588/files/88733489?module_item_id=17073646) method returns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uTgPDrrfwl-U"},"outputs":[],"source":["def build_model(num_features, learning_rate):\n","  \"\"\"Build a TF linear regression model using Keras.\n","\n","  Args:\n","    num_features: The number of input features.\n","    learning_rate: The desired learning rate for SGD.\n","\n","  Returns:\n","    model: A tf.keras model (graph).\n","  \"\"\"\n","  # This is not strictly necessary, but each time you build a model, TF adds\n","  # new nodes (rather than overwriting), so the colab session can end up\n","  # storing lots of copies of the graph when you only care about the most\n","  # recent. Also, as there is some randomness built into training with SGD,\n","  # setting a random seed ensures that results are the same on each identical\n","  # training run.\n","  tf.keras.backend.clear_session()\n","  tf.random.set_seed(0)\n","\n","  # Build a model using keras.Sequential. While this is intended for neural\n","  # networks (which may have multiple layers), we want just a single layer for\n","  # binary logistic regression.\n","\n","  model = tf.keras.Sequential()\n","  model.add(tf.keras.layers.Dense(\n","      units=10,        # output dim\n","      input_shape=(num_features,),  # input dim\n","      use_bias=True,               # use a bias (intercept) param\n","      activation='softmax',\n","      kernel_initializer=tf.keras.initializers.Ones(),  # initialize params to 1\n","      bias_initializer=tf.keras.initializers.Ones()    # initialize bias to 1\n","  ))\n","\n","\n","  # We need to choose an optimizer. We'll use SGD, which is actually mini-batch SGD\n","  optimizer = tf.keras.optimizers.SGD(learning_rate = learning_rate)\n","\n","  # Finally, compile the model. Select the accuracy metric. This finalizes the graph for training.\n","  # Telling our model how it should learn and evaluate its performance\n","  # The optimizer tells the model how to adjust its weigths during training\n","  # The loss measures how well the model's predictions match the actual labels\n","  # The metrics monitor the performance of the model during and after training\n","  model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wlht9Q7iwl-V"},"outputs":[],"source":["tf.random.set_seed(0)\n","\n","# One-hot encode the labels using TensorFlow's built-in functionality\n","Y_train_mini_one_hot = tf.keras.utils.to_categorical(Y_train_mini, num_classes=10)\n","Y_val_one_hot = tf.keras.utils.to_categorical(Y_val, num_classes=10)\n","\n","# Flatten the input data\n","X_train_mini_flat = X_train_mini.reshape(X_train_mini.shape[0], -1)\n","X_val_flat = X_val.reshape(X_val.shape[0], -1)\n","num_features = X_train_mini_flat.shape[1]\n","\n","# Build and compile the model\n","model_tf = build_model(num_features=num_features, learning_rate=0.001)  # Adjusted learning rate\n","\n","# Fit the model\n","history = model_tf.fit(X_train_mini_flat, Y_train_mini_one_hot, epochs=5, batch_size=32, validation_data=(X_val_flat, Y_val_one_hot))\n","\n","# Evaluate the model\n","train_loss, train_accuracy = model_tf.evaluate(X_train_mini_flat, Y_train_mini_one_hot, verbose=1)\n","print(f\"Aggregate accuracy on the mini train dataset: {train_accuracy:.4f}\")\n","\n","test_loss, test_accuracy = model_tf.evaluate(X_val_flat, Y_val_one_hot, verbose=1)\n","print(f\"Aggregate accuracy on the test dataset: {test_accuracy:.4f}\")"]},{"cell_type":"code","source":["# Extract the loss history\n","train_loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","# Plotting the loss values\n","plt.figure(figsize=(10, 6))\n","plt.plot(range(1, 6), train_loss, label='Training Loss')\n","plt.plot(range(1, 6), val_loss, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss Over Epochs')\n","plt.legend()\n","plt.show()\n","\n","print(f\"The training loss is: {train_loss[-1]}\")\n","print(f\"The validation loss is: {val_loss[-1]}\")\n"],"metadata":{"id":"0F-ZMHiRQZnT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IGWIPrqcwl-V"},"source":["---\n","### Step 5: Hyperparameter tuning"]},{"cell_type":"markdown","metadata":{"id":"sLJ-HRbYwl-V"},"source":["Hyperparameter tuning is a crucial step in optimizing ML models. It involves systematically adjusting hyperparameters such as learning rate, number of epochs, and optimizer to find the model configuration that leads to the best generalization performance.\n","\n","This tuning process is typically conducted by monitoring the model's performance on the validation vs. training set. It's important to note that using the test set for hyperparameter tuning can compromise the integrity of the evaluation process by violating the assumption of \"blindness\" of the test data."]},{"cell_type":"markdown","metadata":{"id":"ye_FNcAuwl-V"},"source":["### <span style=\"color:chocolate\">Exercise 10:</span> Hyperparameter tuning (10 points)"]},{"cell_type":"markdown","metadata":{"id":"GdJq_o_twl-V"},"source":["1. Fine-tune the hyperparameters of *model_tf* to determine the setup that yields the most optimal performance. Feel free to explore various values for the hyperparameters. Hint: ask your instructors and TAs for help if in doubt.\n","\n","After identifying your preferred model configuration, print the following information:\n","\n","2. The first five learned parameters of the model (this should include the bias term);\n","3. The loss at the final epoch on both the mini training and validation datasets;\n","4. The percentage difference between the losses observed on the mini training and validation datasets.\n","5. Compare the training/validation loss of the TensorFlow model (model_tf) with the baseline model's loss. Does the TensorFlow model demonstrate an improvement over the baseline model?\n","\n","\n","Please note that we will consider 'optimal model configuration' any last-epoch loss that is below 0.08."]},{"cell_type":"code","source":["tf.random.set_seed(0)\n","# 2. Build and compile model Get the number of features\n","# Flatten the input data\n","\n","model_tf = build_model(num_features=num_features, learning_rate = 0.007)\n","\n","# Fit the model\n","history = model_tf.fit(X_train_mini_flat, Y_train_mini_one_hot, epochs=50, batch_size=12, validation_data=(X_val_flat, Y_val_one_hot))\n"],"metadata":{"id":"ugbGEVSAMEVL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract the loss history\n","train_loss2 = history.history['loss']\n","val_loss2 = history.history['val_loss']\n","\n","# Plotting the loss values\n","plt.figure(figsize=(10, 6))\n","plt.plot(range(1, 51), train_loss2, label='Training Loss')\n","plt.plot(range(1, 51), val_loss2, label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss Over Epochs')\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"SHTYotEwMOWT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract the weights and biases\n","weight_array = model_tf.get_weights()\n","\n","weights = weight_array[0]  # Weights between input and first layer\n","bias = weight_array[1]     # Biases of the first layer\n","\n","# Print the first five weights and the bias term\n","print(\"First five learned parameters of the model (including the bias term):\")\n","\n","# Print the first 5 weights of the first 5 input features for each class\n","for i in range(5):  # Loop through the first 5 input features\n","    print(f\"Weights for input feature {i+1}:\")\n","    for class_index in range(10):  # Loop through all 10 classes\n","        print(f\"  Class {class_index}: {weights[i, class_index]}\")\n","\n","print(\"Biases for each class:\")\n","print(bias)"],"metadata":{"id":"mj5aWnKCQeQQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["3. The loss at the final epoch on both the mini training and validation datasets;\n","4. The percentage difference between the losses observed on the mini training and validation datasets.\n","5. Compare the training/validation loss of the TensorFlow model (model_tf) with the baseline model's loss. Does the TensorFlow model demonstrate an improvement over the baseline model?"],"metadata":{"id":"P8a2mWWhStKF"}},{"cell_type":"code","source":["# 3. Output the loss at the final epoch on both the mini training and validation datasets;\n","print(f\"The loss at the final epoch on the mini training dataset: {train_loss2[-1]}\")\n","print(f\"The loss at the final epoch on the validation training dataset: {val_loss2[-1]}\")"],"metadata":{"id":"tm-ydDC1SuK_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Output the percentage difference between the losses observed on the mini training and validation datasets.\n","# Calculate the absolute difference\n","final_train_loss = train_loss2[-1]\n","final_val_loss = val_loss2[-1]\n","\n","absolute_difference = abs(final_val_loss - final_train_loss)\n","\n","# Calculate the average loss\n","average_loss = (final_val_loss + final_train_loss) / 2\n","\n","# Calculate the percentage difference\n","percentage_difference = (absolute_difference / average_loss) * 100\n","\n","# Print the percentage difference\n","print(f\"The percentage difference between the losses observed on the mini training and validation datasets is: {percentage_difference:.2f}%\")"],"metadata":{"id":"xWYdIwagTJQ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Baseline model losses\n","baseline_train_loss = 1.3868722915649414\n","baseline_val_loss = 1.268518090248108\n","\n","# TensorFlow model losses\n","final_train_loss = train_loss[-1]\n","final_val_loss = val_loss[-1]\n","\n","# Output the losses\n","print(f\"The training loss of the baseline is: {baseline_train_loss}\")\n","print(f\"The validation loss of the baseline is: {baseline_val_loss}\")\n","\n","# Calculate the absolute difference for the baseline model\n","baseline_absolute_difference = abs(baseline_val_loss - baseline_train_loss)\n","\n","# Calculate the average loss for the baseline model\n","baseline_average_loss = (baseline_val_loss + baseline_train_loss) / 2\n","\n","# Calculate the percentage difference for the baseline model\n","baseline_percentage_difference = (baseline_absolute_difference / baseline_average_loss) * 100\n","\n","# Print the percentage difference for the baseline model\n","print(f\"The percentage difference between the losses observed on the baseline model is: {baseline_percentage_difference:.2f}%\")\n","\n","# Compare the training/validation loss of the TensorFlow model with the baseline model's loss\n","print(f\"The training loss of the TensorFlow model is: {final_train_loss}\")\n","print(f\"The validation loss of the TensorFlow model is: {final_val_loss}\")\n","\n","# Check if the TensorFlow model demonstrates an improvement over the baseline model\n","if final_val_loss < baseline_val_loss:\n","    print(\"The TensorFlow model demonstrates an improvement over the baseline model.\")\n","else:\n","    print(\"The TensorFlow model does not demonstrate an improvement over the baseline model.\")\n"],"metadata":{"id":"lVGEdMPgYXAM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R-AQ1Qcowl-V"},"source":["---\n","### Step 6: Evaluation and Generalization"]},{"cell_type":"markdown","metadata":{"id":"M_4fDR_fwl-V"},"source":["\n","Now that you've determined the optimal set of hyperparameters, it's time to evaluate your optimized model on the test data to gauge its performance in real-world scenarios, commonly known as inference."]},{"cell_type":"markdown","metadata":{"id":"K9FC2aX0wl-V"},"source":["### <span style=\"color:chocolate\">Exercise 11:</span> Computing accuracy (10 points)"]},{"cell_type":"markdown","metadata":{"id":"S1zNFjzPwl-W"},"source":["1. Calculate aggregate accuracy on both mini train and test datasets using a probability threshold of 0.5. Hint: You can utilize the <span style=\"color:chocolate\">model.evaluate()</span> method provided by tf.keras. Note: Aggregate accuracy measures the overall correctness of the model across all classes in the dataset;\n","\n","2. Does the model demonstrate strong aggregate generalization capabilities? Provide an explanation based on your accuracy observations."]},{"cell_type":"code","source":["from tensorflow.keras.utils import to_categorical\n","\n","# One-hot encode the test labels\n","Y_test_one_hot = to_categorical(Y_test, num_classes=10)\n","\n","# Flatten the test data to match the shape expected by the model\n","X_test_flat = X_test.reshape(X_test.shape[0], -1)\n","\n","# Evaluate the model using model.evaluate()\n","train_loss, train_accuracy = model_tf.evaluate(X_train_mini_flat, Y_train_mini_one_hot, verbose=1)\n","print(f\"Aggregate accuracy on the mini train dataset (from evaluate): {train_accuracy:.4f}\")\n","\n","test_loss, test_accuracy = model_tf.evaluate(X_test_flat, Y_test_one_hot, verbose=1)\n","print(f\"Aggregate accuracy on the test dataset (from evaluate): {test_accuracy:.4f}\")\n"],"metadata":{"id":"h-dPZEpRAXNf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Does the model demonstrate strong aggregate generalization capabilities? Provide an explanation based on your accuracy observations.\n","\n","No unfortunetly it does really poorly on the test, so much so that I thought I did the whole thing wrong. It has poor generalization and is overfitted.\n","\n"],"metadata":{"id":"8bm4WPJtvxqU"}},{"cell_type":"markdown","metadata":{"id":"WekBCSLYwl-W"},"source":["### <span style=\"color:chocolate\">Exercise 12:</span> Fairness evaluation (10 points)"]},{"cell_type":"markdown","metadata":{"id":"AeSOqWuowl-W"},"source":["1. Generate and visualize the confusion matrix on the test dataset using a probability threshold of 0.5. Additionally, print the True Positives (TP), False Negatives (FN), False Positives (FP), and True Negatives (TN). Hint: you can utilize the <span style=\"color:chocolate\">model.predict()</span> method available in tf.keras, and then the <span style=\"color:chocolate\">confusion_matrix()</span>, <span style=\"color:chocolate\">ConfusionMatrixDisplay()</span> methods available in sklearn.metrics;\n","\n","2. Compute subgroup accuracy, separately for the sneaker and non-sneaker classes, on the test dataset using a probability threshold of 0.5. Reflect on any observed accuracy differences (potential lack of fairness) between the two classes.\n","\n","3. Does the model demonstrate strong subgroup generalization capabilities? Provide an explanation based on your accuracy observations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UqLEB9bLwl-W"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n","\n","# Assuming the model and data are already defined and trained\n","# X_test_flat, Y_test, and model_tf should be available in the context\n","\n","# Predict probabilities\n","test_probs = model_tf.predict(X_test_flat)\n","\n","# Convert probabilities to class labels using a threshold of 0.5\n","test_preds = np.argmax(test_probs, axis=1)\n","\n","# Generate the confusion matrix\n","cm = confusion_matrix(Y_test, test_preds)\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","disp.plot()\n","plt.show()\n","\n","# Calculate TP, FN, FP, TN for each class\n","TP = cm.diagonal()\n","FN = cm.sum(axis=1) - TP\n","FP = cm.sum(axis=0) - TP\n","TN = cm.sum() - (TP + FN + FP)\n","\n","# Print TP, FN, FP, TN for each class\n","for i in range(10):\n","    print(f\"Class {i}:\")\n","    print(f\"  TP: {TP[i]}\")\n","    print(f\"  FN: {FN[i]}\")\n","    print(f\"  FP: {FP[i]}\")\n","    print(f\"  TN: {TN[i]}\")\n","\n","# Compute subgroup accuracy for the sneaker (class 7) and non-sneaker classes\n","sneaker_class = 7\n","non_sneaker_classes = [i for i in range(10) if i != sneaker_class]\n","\n","# Sneaker accuracy\n","sneaker_indices = (Y_test == sneaker_class)\n","sneaker_accuracy = accuracy_score(Y_test[sneaker_indices], test_preds[sneaker_indices])\n","print(f\"Sneaker class accuracy: {sneaker_accuracy:.4f}\")\n","\n","# Non-sneaker accuracy\n","non_sneaker_indices = (Y_test != sneaker_class)\n","non_sneaker_accuracy = accuracy_score(Y_test[non_sneaker_indices], test_preds[non_sneaker_indices])\n","print(f\"Non-sneaker class accuracy: {non_sneaker_accuracy:.4f}\")\n","\n","# Reflect on observed accuracy differences\n","accuracy_difference = sneaker_accuracy - non_sneaker_accuracy\n","print(f\"Accuracy difference between sneaker and non-sneaker classes: {accuracy_difference:.4f}\")\n","\n","# Does the model demonstrate strong subgroup generalization capabilities?\n","if abs(accuracy_difference) < 0.05:\n","    print(\"The model demonstrates strong subgroup generalization capabilities.\")\n","else:\n","    print(\"The model does not demonstrate strong subgroup generalization capabilities.\")\n"]},{"cell_type":"markdown","metadata":{"id":"Jr6oA7Qkwl-W"},"source":["----\n","### <span style=\"color:chocolate\">Bonus question</span> (20 points)"]},{"cell_type":"markdown","metadata":{"id":"RYniGil-wl-W"},"source":["Is it possible to enhance the prediction accuracy for the sneaker class by performing the following steps?\n","\n","1. Implement data balancing techniques, such as oversampling or undersampling, to equalize the representation of both classes.\n","2. After balancing the data, retrain the model on the balanced dataset.\n","3. Evaluate the model's performance, particularly focusing on the accuracy achieved for the sneaker class.\n","\n","Note: provide a separate notebook for the Bonus exercise. Name it ``04 Logistic Regression with Tensorflow_bonus``."]},{"cell_type":"code","source":["from imblearn.over_sampling import RandomOverSampler\n","\n","# Retrain the Model on the Balanced Dataset\n","\n","# Assuming X_train and Y_train are already loaded\n","# Reshape and normalize data\n","X_train_flat = X_train.reshape(X_train.shape[0], -1) / 255.0\n","\n","# Balance the dataset using RandomOverSampler\n","ros = RandomOverSampler(sampling_strategy='auto')\n","X_train_balanced, Y_train_balanced = ros.fit_resample(X_train_flat, Y_train)\n","\n","# One-hot encode the balanced labels\n","Y_train_balanced_one_hot = to_categorical(Y_train_balanced, num_classes=10)\n","\n","# Print the new class distribution\n","print(\"Class distribution after balancing:\")\n","print(np.bincount(Y_train_balanced))"],"metadata":{"id":"i5cNi2tdx0TE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model's performance,\n","# Define the logistic regression model\n","def build_model(num_features, learning_rate):\n","    tf.keras.backend.clear_session()\n","    tf.random.set_seed(0)\n","\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Dense(10, activation='softmax', input_shape=(num_features,))\n","    ])\n","\n","    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    return model\n","\n","# Build and compile the model\n","num_features = X_train_balanced.shape[1]\n","model_tf_balanced = build_model(num_features, learning_rate=0.01)\n","\n","# Train the model\n","history_balanced = model_tf_balanced.fit(X_train_balanced, Y_train_balanced_one_hot, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n","\n","from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n","\n","# Predict on the test set\n","X_test_flat = X_test.reshape(X_test.shape[0], -1) / 255.0\n","Y_test_one_hot = to_categorical(Y_test, num_classes=10)\n","test_probs_balanced = model_tf_balanced.predict(X_test_flat)\n","test_preds_balanced = np.argmax(test_probs_balanced, axis=1)\n","\n","# Generate the confusion matrix\n","cm_balanced = confusion_matrix(Y_test, test_preds_balanced)\n","disp_balanced = ConfusionMatrixDisplay(confusion_matrix=cm_balanced)\n","disp_balanced.plot()\n","plt.show()\n","\n","# Calculate accuracy for sneaker and non-sneaker classes\n","sneaker_accuracy_balanced = accuracy_score(Y_test[Y_test == 7], test_preds_balanced[Y_test == 7])\n","non_sneaker_accuracy_balanced = accuracy_score(Y_test[Y_test != 7], test_preds_balanced[Y_test != 7])\n","\n","print(f\"Sneaker class accuracy: {sneaker_accuracy_balanced:.4f}\")\n","print(f\"Non-sneaker class accuracy: {non_sneaker_accuracy_balanced:.4f}\")\n","\n","# Print the overall accuracy\n","test_loss_balanced, test_accuracy_balanced = model_tf_balanced.evaluate(X_test_flat, Y_test_one_hot, verbose=1)\n","print(f\"Overall test accuracy: {test_accuracy_balanced:.4f}\")"],"metadata":{"id":"H_JZoa3gx8Co"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}