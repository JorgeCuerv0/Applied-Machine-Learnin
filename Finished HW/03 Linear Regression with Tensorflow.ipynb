{"cells":[{"cell_type":"markdown","metadata":{"id":"MKsRDH5ZUdfasdv"},"source":["# Assignment 3"]},{"cell_type":"markdown","metadata":{"id":"nzOfVu6How01"},"source":["### <span style=\"color:chocolate\"> Submission requirements </span>\n","\n","Your work will not be graded if your notebook doesn't include output. In other words, <span style=\"color:red\"> make sure to rerun your notebook before submitting to Gradescope </span> (Note: if you are using Google Colab: go to Edit > Notebook Settings  and uncheck Omit code cell output when saving this notebook, otherwise the output is not printed).\n","\n","Additional points may be deducted if these requirements are not met:\n","    \n","* Comment your code;\n","* Each graph should have a title, labels for each axis, and (if needed) a legend. Each graph should be understandable on its own;\n","* Try and minimize the use of the global namespace (meaning, keep things inside functions).\n","---"]},{"cell_type":"markdown","metadata":{"id":"bB8B75Chow02"},"source":["### Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cJ7GRMSeow02"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns  # for nicer plots\n","sns.set(style=\"darkgrid\")  # default style\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.base import BaseEstimator, RegressorMixin"]},{"cell_type":"markdown","metadata":{"id":"ejdmL5R8ow03"},"source":["This lab continues our study of linear regression. You'll train your first models with Tensorflow, using a real dataset to predict car prices from their features. Note that Tensorflow is a rapidly changing library. This means you'll often see warnings about deprecations. You can ignore the warnings in our labs."]},{"cell_type":"markdown","metadata":{"id":"tsvku_1qow03"},"source":["---\n","### Step 1: Data ingestion"]},{"cell_type":"markdown","metadata":{"id":"rHLcriKWLRe4"},"source":["You'll use the [Automobile Data Set](https://archive.ics.uci.edu/ml/datasets/automobile)  from 1985 Ward's Automotive Yearbook that is part of the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"load_auto_data_set_code"},"outputs":[],"source":["# Provide the names for the feature columns since the CSV file with the data\n","# does not have a header row.\n","cols = ['symboling', 'losses', 'make', 'fuel-type', 'aspiration', 'num-doors',\n","        'body-style', 'drive-wheels', 'engine-location', 'wheel-base',\n","        'length', 'width', 'height', 'weight', 'engine-type', 'num-cylinders',\n","        'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-ratio',\n","        'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\n","\n","# Load the data from a CSV file into a pandas dataframe. Remember that each row\n","# is an example and each column in a feature.\n","car_data_init = pd.read_csv(\n","    'https://storage.googleapis.com/ml_universities/cars_dataset/cars_data.csv',\n","    sep=',', names=cols, header=None, encoding='latin-1')\n","\n","# Display top five rows\n","print('Shape of data:', car_data_init.shape)\n","car_data_init.head()"]},{"cell_type":"markdown","metadata":{"id":"oU5kVpSlow03"},"source":["---\n","### Step 2: Data preprocessing"]},{"cell_type":"markdown","metadata":{"id":"t7EvdMgkow04"},"source":["This step is essential for preparing the data in a format that is suitable for ML algorithms. It helps ensure data quality and improvements in model performance."]},{"cell_type":"markdown","metadata":{"id":"Lv48OhUxow04"},"source":["### <span style=\"color:chocolate\">Exercise 1:</span> Column selection (5 points)"]},{"cell_type":"markdown","metadata":{"id":"JQcMFbsTow04"},"source":["To keep things simple, you will:\n","\n","1. Retain only the following columns: ['horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']. Name the new dataframe *car_data*.\n","2. Display the data type of each column;\n","3. Convert the data type of each columns to numeric. Coerce missing values to NaN. Hint: use <span style=\"color:chocolate\">pd.to_numeric()</span> method;\n","4. Display the data type of each column after the transformation performed at point 3."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-H9qdnueow04"},"outputs":[],"source":["# made a list of column names\n","cols_to_keep = ['horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\n","\n","# made a new df with only the columns that we want by using our list as a filter\n","car_data = car_data_init[cols_to_keep]\n","\n","# output type of data each column holds\n","print(car_data.dtypes)\n","\n","# used the apply method to \"apply\" to_numeric to every column of our df and turned any error in 'Nan\"\n","car_data = car_data.apply(pd.to_numeric, errors='coerce')\n","\n","# output type of data each column holds\n","print(car_data.dtypes)"]},{"cell_type":"markdown","metadata":{"id":"CielXQlkow04"},"source":["### <span style=\"color:chocolate\">Exercise 2:</span> Example (row) selection (5 points)"]},{"cell_type":"markdown","metadata":{"id":"przjOtmUow05"},"source":["To keep things simple again, you will:\n","\n","1. Print the shape of the car_data;\n","\n","2. Remove examples (rows) that have missing value(s). Note that in doing so, you will overwrite the car_data dataset. You should end up with 199 examples after this cleaning.\n","\n","3. Print the shape of the car_data again.\n","\n","It's important to acknowledge that there are multiple approaches to handling missing features, and simply discarding examples with any missing feature, though straightforward, may not be the most optimal solution. However, for the sake of simplicity, you will implement this strategy in this assignment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bja_C6A6ow05"},"outputs":[],"source":["# print car_data shape\n","print(car_data.shape)\n","\n","# remove example rows with Nan values\n","cleaned_car_data = car_data.dropna()\n","\n","# print car_data shape\n","print(cleaned_car_data.shape)"]},{"cell_type":"markdown","metadata":{"id":"A-3E9D9low05"},"source":["### <span style=\"color:chocolate\">Exercise 3:</span> Data shuffling (10 points)"]},{"cell_type":"markdown","metadata":{"id":"Q0wTjLjfow05"},"source":["Since you'll be using Batch Gradient Descent (BGD) for training, it is important that **each batch is a random sample of the data** so that the gradient computed is representative. Note that the original data (above) appears sorted by *make* in alphabetic order."]},{"cell_type":"markdown","metadata":{"id":"uqA1joRjow05"},"source":["Using NumPy and Pandas methods:\n","\n","1. Create a list of indices corresponding to the rows in the car_data dataset. Call this list *indices*. Print this list;\n","\n","2. Shuffle *indices* using the <span style=\"color:chocolate\">np.random.permutation()</span> method. Call the resulting array *shuffled_indices*. Print this array;\n","    \n","3. Use the method <span style=\"color:chocolate\">dataframe.reindex()</span> to change the ordering of the car_data dataset based on the order in the *shuffled_indices* array. Note that in doing so, you will overwrite the original dataset. Print the top 5 rows."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xdZ4QyYnow05"},"outputs":[],"source":["np.random.seed(0)\n","# Create a list of indices corresponding to the rows in df\n","indices = cleaned_car_data.index.tolist()\n","\n","#note that rows keep their original indice number even after rows are dropped\n","print(indices)\n","\n","# we will shuffel the list of lindices \"randomly\"\n","shuffled_indices = np.random.permutation(indices)\n","\n","print(shuffled_indices)\n","\n","# we made a new df and set that equal to cleaned_car_data but ordered by the random indeces we just shuffeled\n","indieced_clened_car_data = cleaned_car_data.reindex(shuffled_indices)\n","\n","print(indieced_clened_car_data.head())"]},{"cell_type":"markdown","metadata":{"id":"iHjRwMzpow05"},"source":["### <span style=\"color:chocolate\">Exercise 4:</span> Define outcome and features (5 points)"]},{"cell_type":"markdown","metadata":{"id":"napOAmrTow05"},"source":["Create two dataframes as follows:\n","\n","1. The first dataframe contains our outcome of interest: ['price']. Note, this is what we are aiming to predict. Name this dataframe Y. Print shape of Y.\n","2. The second dataframe contains our features of interest: ['horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg']. Name this dataframe X. Print shape of X.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8EW-lMwXow06"},"outputs":[],"source":["#  Created a new df containing prices\n","Y = indieced_clened_car_data['price']\n","print(Y.shape)\n","\n","#  Created a new df containing 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg'\n","X = indieced_clened_car_data[['horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg']]\n","print(X.shape)"]},{"cell_type":"markdown","metadata":{"id":"gdc6_Weuow06"},"source":["### <span style=\"color:chocolate\">Exercise 5:</span> Data splits (10 points)"]},{"cell_type":"markdown","metadata":{"id":"YM9JwNE7ow06"},"source":["Using the <span style=\"color:chocolate\">train_test_split()</span> method available in scikit-learn:\n","1. Partition the (X, Y) data into training, validation, and test sets using a splitting rule of [60%, 20%, 20%], with a random state set to 1234. Name the resulting dataframes as follows: X_train, X_val, X_test, Y_train, Y_val, Y_test. Hint: To create these three partitions you will utilize the train_test_split() method twice. You should obtain [119, 40, 40] examples for training, validation, and test, respectively.\n","2. Print the shape of each dataframe.\n","\n","Note: The validation set is crucial for evaluating different hyperparameter configurations and selecting those that yield optimal model performance. This approach avoids utilizing the test dataset during model training, as it is assumed to be \"unknown\" at that stage."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9iSdPkntow06"},"outputs":[],"source":["# YOUR CODE HERE\n","X_train_val, X_test, Y_train_val, Y_test =  train_test_split(X, Y, test_size = 0.2, random_state = 1234)\n","\n","#25% of 80% = 20% of total\n","X_train, X_val, Y_train, Y_val =  train_test_split(X_train_val, Y_train_val, test_size = 0.25, random_state = 1234)\n","\n","print(X_train.shape)\n","print(X_val.shape)\n","print(X_test.shape)\n","print(Y_train.shape)\n","print(Y_val.shape)\n","print(Y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"XXcRIhBKow06"},"source":["### <span style=\"color:chocolate\">Exercise 6:</span> Data standardization (10 points)"]},{"cell_type":"markdown","metadata":{"id":"TUdH2TPZow06"},"source":["With this concept in mind, complete the following tasks:\n","\n","1. Output the quantile values (0.25, 0.5, 0.75, 0.95) for all features in the X_train dataset. Are these values uniformly scaled across features?\n","\n","2. Standardize all features in X_train, X_val, and X_test. Label the resulting dataframes as X_train_std, X_val_std, and X_test_std, respectively. Hint: standardize the validation and test data using the mean and standard deviation computed from the training data. Why?\n","\n","3. Similar to point 2. but now standardize the outcome variable. Label the resulting dataframes as Y_train_std, Y_val_std, and Y_test_std."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wk45U21jow06"},"outputs":[],"source":["# Used the quantile method to output quantile values of all features\n","quantiles = X_train.quantile([0.25, 0.5, 0.75, 0.95])\n","print(quantiles)\n","\n","# Standardize all features so validation data is scaled in the same way as our training data.\n","mean = X_train.mean()\n","std = X_train.std()\n","\n","X_train_std = ( X_train - mean) / std\n","X_val_std = ( X_val - mean) / std\n","X_test_std = ( X_test - mean) / std\n","\n","Y_mean = Y_train.mean()\n","Y_std = Y_train.std()\n","\n","Y_train_std = (Y_train - Y_mean) / Y_std\n","Y_val_std = (Y_val - Y_mean) / Y_std\n","Y_test_std = (Y_test - Y_mean) / Y_std\n","\n","# We standardize all the data because we need to have them usung the same scale to help maintain the relationship between the data points, otherwise it would be like comparing apples to oranges"]},{"cell_type":"markdown","metadata":{"id":"vAPGVCJAow06"},"source":["---\n","### Step 3: Exploratory data analysis (EDA)"]},{"cell_type":"markdown","metadata":{"id":"ypM3Qgb4ow07"},"source":["EDA plays a very important role in ML. The goal here is to develop a good understanding of our dataset, identify any data quality issues, understand patterns and relationships, which in turn, aids in subsequent modeling and interpretations."]},{"cell_type":"markdown","metadata":{"id":"i6aq4M80ow07"},"source":["### <span style=\"color:chocolate\">Exercise 7:</span> Scatterplot matrix (10 points)"]},{"cell_type":"markdown","metadata":{"id":"INwbDe_Fow07"},"source":["In this exercise you will use some simple yet useful techniques to visualize the distribution of the data.\n","\n","Let's start with:\n","\n","1. A scatterplot matrix to visualize the pair-wise correlations between different features and outcome in the (X_train_std, Y_train_std) data. You will use the <span style=\"color:chocolate\">sns.pairplot()</span> method from the seaborn library imported at the top of the notebook;\n","2. Is any of the variables in the data normally distributed? Is it necessary for the explanatory or target variable to be normally distributed in order to train a ML model?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_cm4vAw7ow07"},"outputs":[],"source":["# Make a data frame with the X_train_std, Y_train_std data we just created\n","train_std = X_train_std.copy()\n","train_std['Y'] = Y_train_std\n","\n","#plot\n","sns.pairplot(train_std)\n","\n","# In our data, none of the variables seem to be normally distributed. Fortunately for us, this is not a prerequisite for the explanatory or target variable to be normally distributed in order to train our ML model"]},{"cell_type":"markdown","metadata":{"id":"JIlt-eblow07"},"source":["### <span style=\"color:chocolate\">Exercise 8:</span> Correlation matrix (10 points)"]},{"cell_type":"markdown","metadata":{"id":"SwzgMhumow07"},"source":["In this exercise you will:\n","\n","1. Plot a correlation matrix in the form of a heatmap to visualize the linear relationships between different features and outcome in the (X_train_std, Y_train) data. Hint: this example here is very useful: https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n","    \n","2. Answer the following questions:\n"," - Which two features are likely to be most redundant?\n"," - Which feature is likely to be least useful for predicting price?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tiWBQhubow09"},"outputs":[],"source":["# Plotting correlation matrix with data from the X_train_std, Y_train data\n","correlation_df = X_train_std.copy()\n","correlation_df['Y'] = Y_train\n","\n","# Compute the correlation matrix\n","corr = correlation_df.corr()\n","\n","# Generate a mask for the upper triangle\n","mask = np.triu(np.ones_like(corr, dtype=bool))\n","\n","sns.heatmap(corr, annot = True, fmt = '.1f', mask = mask, linewidths = 0.05, vmax=.3, center=0, cmap = 'coolwarm')\n","\n","# Which two features are likely to be most redundant? - high correlation with each other, indicating that they provide similar information. -\n","#city-mpg and highway-mpg are most redundant because of theri score of -.8\n","# Which feature is likely to be least useful for predicting price? - look for the feature with the lowest absolute correlation with Y. -\n","# the frature least likely to be useful for predicting price is peak-rpm: This feature has a correlation of -0.1"]},{"cell_type":"markdown","metadata":{"id":"Lphup98Tow09"},"source":["---\n","### Step 4: Modeling"]},{"cell_type":"markdown","metadata":{"id":"ChvJvZ8dow09"},"source":["### <span style=\"color:chocolate\">Exercise 9:</span> Baseline model (5 points)"]},{"cell_type":"markdown","metadata":{"id":"jLgCWSrGow0-"},"source":["Let's start by evaluating a baseline model. Precisely, you'll use the average price of cars in the training set as our baseline model -- that is, the baseline always predicts the average price regardless of the input.\n","\n","1. Implement this baseline using the Y_train_std data and print the average price. Note: You can revert the price variable to the original scale for interpretation purposes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cnJuPThhow0-"},"outputs":[],"source":["# YOUR CODE HERE\n","average_price_std = Y_train_std.mean()\n","\n","Y_ave_price = (average_price_std * Y_std)  + Y_mean\n","\n","#create a list where each element is the average price\n","baseline_predictions = [Y_ave_price] * len(Y_test)\n","\n","print(baseline_predictions)"]},{"cell_type":"markdown","metadata":{"id":"osBXeXWygp4T"},"source":["### <span style=\"color:chocolate\">Exercise 10:</span> Improvement over Baseline with TensorFlow (10 points)"]},{"cell_type":"markdown","metadata":{"id":"vDsxLnljlp0C"},"source":["Let's train a linear regression model much like we did in the previous assignment, but this time using TensorFlow.\n","\n","1. Fill in the <span style=\"color:green\">NotImplemented</span> parts of the build_model() function below by following the instructions provided as comments. Hint: refer to Demo 3 in [bCourses/Modules/Live Session Demos](https://bcourses.berkeley.edu/courses/1534588/files/88733489?module_item_id=17073646) for an example.\n","2. Build and compile a model using the build_model() function and the (X_train_std, Y_train_std) data. Set learning_rate = 0.0001. Call the resulting object *model_tf*.\n","3. Train *model_tf* using the (X_train_std, Y_train_std) data. Set num_epochs = 5. Pass the (X_val_std, Y_val_std) data for validation. Hint: see the documentation behind the [tf.keras.Model.fit()](https://bcourses.berkeley.edu/courses/1534588/files/88733489?module_item_id=17073646) method.\n","3. Generate a plot with the loss values on the y-axis and the epoch number on the x-axis for visualization. Make sure to include axes name and title. Hint: check what the [tf.keras.Model.fit()](https://bcourses.berkeley.edu/courses/1534588/files/88733489?module_item_id=17073646) method returns.\n","\n","More notes on point 1: the idea is to build a *computational graph* for linear regression, and then send data through it. There are many ways to build graphs, but [TenforFlow Keras API](https://www.tensorflow.org/api_docs/python/tf/keras) is recommended."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pfdRzjk-RgpG"},"outputs":[],"source":["def build_model(num_features, learning_rate):\n","  \"\"\"Build a TF linear regression model using Keras.\n","\n","  Args:\n","    num_features: The number of input features.\n","    learning_rate: The desired learning rate for SGD.\n","\n","  Returns:\n","    model: A tf.keras model (graph).\n","  \"\"\"\n","  # This is not strictly necessary, but each time you build a model, TF adds\n","  # new nodes (rather than overwriting), so the colab session can end up\n","  # storing lots of copies of the graph when you only care about the most\n","  # recent. Also, as there is some randomness built into training with SGD,\n","  # setting a random seed ensures that results are the same on each identical\n","  # training run.\n","  tf.keras.backend.clear_session()\n","  tf.random.set_seed(0)\n","\n","  # Build a model using keras.Sequential. While this is intended for neural\n","  # networks (which may have multiple layers), we want just a single layer for\n","  # linear regression.\n","  model = tf.keras.Sequential()\n","  model.add(tf.keras.layers.Dense(\n","      units = 1,        # output dim\n","      input_shape = (num_features,), # input dim\n","      use_bias = True,               # use a bias (intercept) param\n","      kernel_initializer = tf.keras.initializers.ones,  # initialize params to 1\n","      bias_initializer = tf.keras.initializers.ones,    # initialize bias to 1\n","  ))\n","\n","  # We need to choose an optimizer. We'll use GD, which is actually mini-batch GD\n","  optimizer = tf.keras.optimizers.SGD(learning_rate = learning_rate)\n","\n","  # Finally, compile the model. This finalizes the graph for training.\n","  # We specify the loss and the optimizer above\n","  model.compile(optimizer = optimizer, loss = 'mse')\n","\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4XOu5YTow0_"},"outputs":[],"source":["tf.random.set_seed(0)\n","# 2. Build and compile model  with (X_train_std, Y_train_std) data.\n","\n","num_o_features = X_train_std.shape[1]\n","model_tf = build_model(num_features=num_o_features, learning_rate=0.0001)\n","history = model_tf.fit(X_train_std, Y_train_std, batch_size=32, epochs=5, validation_data=(X_val_std, Y_val_std))\n","\n","#4. Generate a plot\n","#extract loss values & val loss\n","training_loss = history.history['loss']\n","val_loss = history.history.get('val_loss')\n","\n","#create a figure\n","plt.figure()\n","\n","#plot loss\n","plt.plot(training_loss, label='Training Loss')\n","\n","if val_loss:\n","    plt.plot(val_loss, label='Validation Loss')\n","\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Training and Validation Loss Over Epochs')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"rWZS9f5Qow0_"},"source":["---\n","### Step 5: Hyperparameter tuning"]},{"cell_type":"markdown","metadata":{"id":"cuHP4k7Aow0_"},"source":["Hyperparameter tuning is a crucial step in optimizing ML models. It involves systematically adjusting hyperparameters such as learning rate, number of epochs, and optimizer to find the model configuration that leads to the best generalization performance.\n","\n","This tuning process is typically conducted by monitoring the model's performance on the validation vs. training set. It's important to note that using the test set for hyperparameter tuning can compromise the integrity of the evaluation process by violating the assumption of \"blindness\" of the test data."]},{"cell_type":"markdown","metadata":{"id":"BfI1roYBow0_"},"source":["### <span style=\"color:chocolate\">Exercise 11:</span> Hyperparameter tuning (10 points)"]},{"cell_type":"markdown","metadata":{"id":"f9_P12sBow0_"},"source":["1. Fine-tune the hyperparameters of *model_tf* to determine the setup that yields the most optimal generalization performance. Feel free to explore various values for the hyperparameters. Hint: ask your instructors and TAs for help if in doubt.\n","\n","After identifying your preferred model configuration, print the following information:\n","\n","2. The learned parameters of the model (this should include the bias term). Hint: use  <span style=\"color:chocolate\">model_tf.layers[0].get_weights()</span>.\n","3. The loss at the final epoch on both the training and validation datasets;\n","4. The percentage difference between the losses observed on the training and validation datasets.\n","\n","\n","Please note that we will consider 'optimal model configuration' any last-epoch loss that is below 0.35."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9FZdzfCow0_"},"outputs":[],"source":["tf.random.set_seed(0)\n","from tqdm import tqdm\n","\n","# YOUR CODE HERE\n","# I didnt want to manually find the best hyperparameters so I looked online how to automate it\n","# Define the refined hyperparameter grid\n","param_grid = {\n","    'learning_rate': [0.01, 0.001, 0.0001, 0.02, 0.002, 0.0002, 0.05, 0.005, 0.0005],\n","    'epochs': [5, 10, 25, 50, 100],\n","    'batch_size': [16, 32, 64]\n","}\n","\n","# Function to build the model\n","def build_model(num_features, learning_rate=0.01):\n","    tf.keras.backend.clear_session()\n","    tf.random.set_seed(0)\n","    model = tf.keras.Sequential()\n","    model.add(tf.keras.layers.Dense(\n","        units=1,\n","        input_shape=(num_features,),\n","        use_bias=True,\n","        kernel_initializer=tf.keras.initializers.Ones(),\n","        bias_initializer=tf.keras.initializers.Ones(),\n","    ))\n","    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='mse')\n","    return model\n","\n","# Scale the data\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train_std)\n","X_val_scaled = scaler.transform(X_val_std)\n","\n","num_features = X_train_std.shape[1]\n","\n","best_params = None\n","best_val_loss = float('inf')\n","best_model = None\n","\n","# Calculate total number of iterations for progress bar\n","total_iterations = len(param_grid['learning_rate']) * len(param_grid['epochs']) * len(param_grid['batch_size'])\n","\n","# Iterate through the hyperparameter grid with a progress bar\n","with tqdm(total=total_iterations) as pbar:\n","    for lr in param_grid['learning_rate']:\n","        for epochs in param_grid['epochs']:\n","            for batch_size in param_grid['batch_size']:\n","                # Build and compile the model\n","                model = build_model(num_features=num_features, learning_rate=lr)\n","\n","                # Train the model\n","                history = model.fit(X_train_scaled, Y_train_std, epochs=epochs, batch_size=batch_size, validation_data=(X_val_scaled, Y_val_std), verbose=0)\n","\n","                # Evaluate the model\n","                train_loss = history.history['loss'][-1]\n","                val_loss = history.history['val_loss'][-1]\n","\n","                # Check if this is the best model\n","                if val_loss < best_val_loss:\n","                    best_val_loss = val_loss\n","                    best_params = {'learning_rate': lr, 'epochs': epochs, 'batch_size': batch_size}\n","                    best_model = model\n","\n","                # Update progress bar\n","                pbar.update(1)\n","\n","# Print the best hyperparameters\n","print(\"Best hyperparameters:\", best_params)\n","\n","# Print the learned parameters\n","weights, biases = best_model.layers[0].get_weights()\n","print(\"Learned weights:\", weights)\n","print(\"Biases:\", biases)\n","\n","# Print the final epoch loss on the training and validation datasets\n","print(\"Training loss:\", train_loss)\n","print(\"Validation loss:\", val_loss)\n","\n","# Calculate the percentage difference between the training and validation losses\n","percentage_difference = abs(train_loss - val_loss) / train_loss * 100\n","print(\"Percentage difference between training and validation loss:\", percentage_difference)\n","\n","# Check if the optimal model configuration is achieved\n","if val_loss < 0.35:\n","    print(\"Optimal model configuration achieved.\")\n","else:\n","    print(\"Optimal model configuration not achieved.\")"]},{"cell_type":"markdown","metadata":{"id":"7jBxVPh_ow1A"},"source":["---\n","### Step 6: Evaluation and Generalization"]},{"cell_type":"markdown","metadata":{"id":"2w9g5lqHow1A"},"source":["\n","Now that you've determined the optimal set of hyperparameters, it's time to evaluate your optimized model on the test data to gauge its performance in real-world scenarios, commonly known as inference."]},{"cell_type":"markdown","metadata":{"id":"98fWNqN3ow1A"},"source":["### <span style=\"color:chocolate\">Exercise 12:</span> Computing MSE (10 points)"]},{"cell_type":"markdown","metadata":{"id":"IZhdAqn9ow1A"},"source":["1. Calculate MSE on both (X_train_std, Y_train_std) and (X_test_std, Y_test_std) datasets. Hint: You can utilize the <span style=\"color:chocolate\">model.evaluate()</span> method provided by tf.keras.\n","\n","2. Does the model demonstrate strong generalization capabilities? Provide an explanation based on your observations.\n","\n","4. Generate a plot to visualize the accuracy of the predictions. Plot the actual (observed) Y_test values on the x-axis and the predicted Y_test values on the y-axis. Additionally, include a 45-degree line in the plot for reference. Ensure that the plot contains appropriate axis labels and a title. Provide commentary on the model's fit based on this visualization. Hint: You can utilize the <span style=\"color:chocolate\">model.predict()</span> method available in tf.keras."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AEK3Bc6eow1A","collapsed":true},"outputs":[],"source":["# YOUR CODE HERE\n","epochs = 100\n","batch_size = 16\n","learning_rate = 0.02\n","\n","best_model = build_model(num_features = num_o_features, learning_rate = learning_rate)\n","best_model.fit(X_train_std, Y_train_std, batch_size=batch_size, epochs=epochs, validation_data=(X_val_std, Y_val_std))\n","\n","train_mse = best_model.evaluate(X_train_std, Y_train_std)\n","print(f\"Training MSE: {train_mse}\")\n","\n","test_mse = best_model.evaluate(X_test_std, Y_test_std)\n","print(f\"Test MSE: {test_mse}\")\n","\n","#this model does demonstrate strong generalization because the mse between training  and test  are close Training MSE: 0.2644072473049164  Test MSE: 0.25394168496131897"]},{"cell_type":"code","source":["# Ensure you predict using the test set\n","Y_pred = best_model.predict(X_test_std)\n","\n","# Flatten the Y_pred array if necessary\n","if Y_pred.shape != Y_test_std.shape:\n","    Y_pred = Y_pred.flatten()\n","\n","# Plot the observed vs predicted values\n","plt.figure(figsize=(8, 6))\n","plt.scatter(Y_test_std, Y_pred, color='blue', alpha=0.5, label='Predicted vs Actual')\n","plt.plot([min(Y_test_std), max(Y_test_std)], [min(Y_test_std), max(Y_test_std)], color='red', linestyle='--', label='45-degree line')\n","\n","# Add labels and title\n","plt.xlabel('Observed Y_test values')\n","plt.ylabel('Predicted Y_test values')\n","plt.title('Accuracy of the Predictions')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Rg-leBRmVA3b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Provide commentary on the model's fit based on this visualization. Hint: You can utilize the model.predict() method available in tf.keras.\n","\n","there is alot of clustering near the origin and as we get further away the predictions start to deviate meaning that there might be a certain price range where our model is really good at predicting. Overall our predictions are near our 45 degree line meaning it  good model performance with some deviations."],"metadata":{"id":"DY3nZPvSaN6J"}},{"cell_type":"markdown","metadata":{"id":"9C5Ydim-ow1A"},"source":["----\n","### <span style=\"color:chocolate\">Bonus question</span> (20 points)"]},{"cell_type":"markdown","metadata":{"id":"446y78jyow1A"},"source":["In Exercise 12, you reported an aggregated MSE. Let's revisit the exercise by:\n","\n","1. Conducting a subgroup model evaluation. More precisely, compute the test data MSE based on various car subgroups such as make, engine size, fuel type, etc.\n","\n","2. Answering the question: is the model \"fair\" to your chosen car subgroups in our data?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gAY-wKOpow1B"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":0}